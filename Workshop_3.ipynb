{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Workshop 3 (Pataranan)",
   "id": "3bf8b103674b2461"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import library",
   "id": "9cd9c9cc749084cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:15.514851Z",
     "start_time": "2024-12-22T18:31:15.504868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "# Download the stopwords resource\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ],
   "id": "57ebfbaf7631cac0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bampatra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean data",
   "id": "e2c531f9e7ba47e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:15.831757Z",
     "start_time": "2024-12-22T18:31:15.524513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the initial dataset\n",
    "file_path = 'resource/spam.csv'\n",
    "data = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Check for NaN values\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "data['cleaned_text'] = data['v2'].astype(str).apply(clean_text)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['v2','Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "\n",
    "# Save the cleaned data\n",
    "output_file_path = 'cleaned_spam_dataset.csv'\n",
    "data.to_csv(output_file_path, index=False)"
   ],
   "id": "afa4755d1822d8fa",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate word length and added with column name “length”",
   "id": "417b9c255b8cbae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:15.882924Z",
     "start_time": "2024-12-22T18:31:15.868050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the cleaned dataset\n",
    "cleaned_data = pd.read_csv(output_file_path)\n",
    "\n",
    "# Ensure 'cleaned_text' column contains strings and handle missing values\n",
    "cleaned_data['cleaned_text'] = cleaned_data['cleaned_text'].fillna('').astype(str)\n",
    "cleaned_data['v1'] = cleaned_data['v1'].fillna('unknown')\n",
    "\n",
    "# Calculate word length for each row in the 'cleaned_text' column and add a new column 'length'\n",
    "cleaned_data['length'] = cleaned_data['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Save the updated dataset\n",
    "length_file_path = 'cleaned_spam_dataset_with_length.csv'\n",
    "cleaned_data.to_csv(length_file_path, index=False)"
   ],
   "id": "615fcbb15e395216",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use labelEncoder method to convert class target",
   "id": "f3960891120be23f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:15.895092Z",
     "start_time": "2024-12-22T18:31:15.885855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use LabelEncoder to convert the class target\n",
    "label_encoder = LabelEncoder()\n",
    "cleaned_data['label'] = label_encoder.fit_transform(cleaned_data['v1'])\n",
    "\n",
    "# Save the final dataset with label encoding\n",
    "label_file_path = 'cleaned_spam_dataset_with_labels.csv'\n",
    "cleaned_data.to_csv(label_file_path, index=False)"
   ],
   "id": "cb08148abca13cff",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use CountVectorize to perform BOW",
   "id": "eb5e398fff2e9939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:22.329748Z",
     "start_time": "2024-12-22T18:31:15.919397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply CountVectorizer to the 'cleaned_text' column\n",
    "bow_matrix = vectorizer.fit_transform(cleaned_data['cleaned_text'])\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a DataFrame for better readability\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Save the Bag-of-Words representation to a CSV file\n",
    "bow_file_path = 'bag_of_words_representation.csv'\n",
    "bow_df.to_csv(bow_file_path, index=False)"
   ],
   "id": "68c838c9863df48c",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### List Top 5 and bottom 5 of transform sample to show the results",
   "id": "a64064284a05e434"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:22.899385Z",
     "start_time": "2024-12-22T18:31:22.358459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract top 5 and bottom 5 transformed samples based on row indices\n",
    "top_5 = bow_df.head(5)\n",
    "bottom_5 = bow_df.tail(5)\n",
    "\n",
    "# Save the results to a text file\n",
    "top_bottom_file_path = 'top_bottom_samples.txt'\n",
    "with open(top_bottom_file_path, 'w') as file:\n",
    "    file.write(\"Top 5 Samples:\\n\")\n",
    "    file.write(top_5.to_string())\n",
    "    file.write(\"\\n\\nBottom 5 Samples:\\n\")\n",
    "    file.write(bottom_5.to_string())"
   ],
   "id": "4b393e1ef21a92dc",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:31:22.918733Z",
     "start_time": "2024-12-22T18:31:22.915773Z"
    }
   },
   "cell_type": "code",
   "source": "(top_bottom_file_path, bow_file_path, length_file_path, label_file_path)",
   "id": "1d817ef3ce6e5a25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('top_bottom_samples.txt',\n",
       " 'bag_of_words_representation.csv',\n",
       " 'cleaned_spam_dataset_with_length.csv',\n",
       " 'cleaned_spam_dataset_with_labels.csv')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
